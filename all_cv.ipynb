{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV Methods for project Distant Viewing of Youtube game play videos\n",
    "\n",
    "## 1 Creating a venv with Python 3.10.x and activate as kernel\n",
    "\n",
    "## 2 Installing required packages\n",
    "- To install all required packages for this pipeline, use the command in `installPackages.sh`\n",
    "- following error can be ignored: `ERROR: Cannot install -r requirements.txt (line 40), google-auth==1.4.2 and tensorboard==2.11.2 because these package versions have conflicting dependencies.`\n",
    "- Installing detectron2 is only required for Object Detection\n",
    "- If you want to run Object Detection, uncomment imports and Object Detection class\n",
    "- Object Detection requires a GPU\n",
    "- takes some time to install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: absl-py==1.4.0 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from -r requirements.txt (line 1)) (1.4.0)\n",
      "Collecting antlr4-python3-runtime==4.9.3\n",
      "  Using cached antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting anyio==3.6.2\n",
      "  Using cached anyio-3.6.2-py3-none-any.whl (80 kB)\n",
      "Collecting argon2-cffi==21.3.0\n",
      "  Using cached argon2_cffi-21.3.0-py3-none-any.whl (14 kB)\n",
      "Collecting argon2-cffi-bindings==21.2.0\n",
      "  Using cached argon2_cffi_bindings-21.2.0-cp36-abi3-win_amd64.whl (30 kB)\n",
      "Collecting arrow==1.2.3\n",
      "  Using cached arrow-1.2.3-py3-none-any.whl (66 kB)\n",
      "Requirement already satisfied: asttokens==2.2.1 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from -r requirements.txt (line 7)) (2.2.1)\n",
      "Requirement already satisfied: astunparse==1.6.3 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from -r requirements.txt (line 8)) (1.6.3)\n",
      "Collecting attrs==22.2.0\n",
      "  Using cached attrs-22.2.0-py3-none-any.whl (60 kB)\n",
      "Requirement already satisfied: backcall==0.2.0 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from -r requirements.txt (line 10)) (0.2.0)\n",
      "Collecting beautifulsoup4==4.11.2\n",
      "  Using cached beautifulsoup4-4.11.2-py3-none-any.whl (129 kB)\n",
      "Collecting black==23.1.0\n",
      "  Using cached black-23.1.0-cp310-cp310-win_amd64.whl (1.2 MB)\n",
      "Collecting bleach==6.0.0\n",
      "  Using cached bleach-6.0.0-py3-none-any.whl (162 kB)\n",
      "Requirement already satisfied: cachetools==5.3.0 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from -r requirements.txt (line 14)) (5.3.0)\n",
      "Collecting certifi==2022.12.7\n",
      "  Using cached certifi-2022.12.7-py3-none-any.whl (155 kB)\n",
      "Collecting cffi==1.15.1\n",
      "  Using cached cffi-1.15.1-cp310-cp310-win_amd64.whl (179 kB)\n",
      "Collecting chardet==3.0.4\n",
      "  Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "Collecting charset-normalizer==3.0.1\n",
      "  Using cached charset_normalizer-3.0.1-cp310-cp310-win_amd64.whl (96 kB)\n",
      "Collecting click==8.1.3\n",
      "  Using cached click-8.1.3-py3-none-any.whl (96 kB)\n",
      "Collecting cloudpickle==2.2.1\n",
      "  Using cached cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: colorama==0.4.6 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from -r requirements.txt (line 21)) (0.4.6)\n",
      "Requirement already satisfied: colorthief==0.2.1 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from -r requirements.txt (line 22)) (0.2.1)\n",
      "Collecting comm==0.1.2\n",
      "  Using cached comm-0.1.2-py3-none-any.whl (6.5 kB)\n",
      "Collecting contourpy==1.0.7\n",
      "  Using cached contourpy-1.0.7-cp310-cp310-win_amd64.whl (162 kB)\n",
      "Requirement already satisfied: cycler==0.11.0 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from -r requirements.txt (line 25)) (0.11.0)\n",
      "Collecting debugpy==1.6.6\n",
      "  Using cached debugpy-1.6.6-cp310-cp310-win_amd64.whl (4.8 MB)\n",
      "Requirement already satisfied: decorator==4.4.2 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from -r requirements.txt (line 27)) (4.4.2)\n",
      "Collecting defusedxml==0.7.1\n",
      "  Using cached defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: entrypoints==0.4 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from -r requirements.txt (line 29)) (0.4)\n",
      "Requirement already satisfied: executing==1.2.0 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from -r requirements.txt (line 30)) (1.2.0)\n",
      "Requirement already satisfied: facenet-pytorch==2.5.2 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from -r requirements.txt (line 31)) (2.5.2)\n",
      "Collecting fastjsonschema==2.16.2\n",
      "  Using cached fastjsonschema-2.16.2-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: fer==22.5.0 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from -r requirements.txt (line 33)) (22.5.0)\n",
      "Requirement already satisfied: flatbuffers==23.1.21 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from -r requirements.txt (line 34)) (23.1.21)\n",
      "Requirement already satisfied: fonttools==4.38.0 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from -r requirements.txt (line 35)) (4.38.0)\n",
      "Collecting fqdn==1.5.1\n",
      "  Using cached fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
      "Collecting fvcore==0.1.5.post20221221\n",
      "  Using cached fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: gast==0.4.0 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from -r requirements.txt (line 38)) (0.4.0)\n",
      "Collecting google-auth==1.4.2\n",
      "  Using cached google_auth-1.4.2-py2.py3-none-any.whl (64 kB)\n",
      "Requirement already satisfied: google-auth-oauthlib==0.4.6 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from -r requirements.txt (line 40)) (0.4.6)\n",
      "Requirement already satisfied: google-pasta==0.2.0 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from -r requirements.txt (line 41)) (0.2.0)\n",
      "Requirement already satisfied: grpcio==1.51.1 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from -r requirements.txt (line 42)) (1.51.1)\n",
      "Requirement already satisfied: h5py==3.8.0 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from -r requirements.txt (line 43)) (3.8.0)\n",
      "Collecting hydra-core==1.3.1\n",
      "  Using cached hydra_core-1.3.1-py3-none-any.whl (154 kB)\n",
      "Collecting idna==2.8\n",
      "  Using cached idna-2.8-py2.py3-none-any.whl (58 kB)\n",
      "Collecting imageio==2.25.0\n",
      "  Using cached imageio-2.25.0-py3-none-any.whl (3.4 MB)\n",
      "Requirement already satisfied: imageio-ffmpeg==0.4.8 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from -r requirements.txt (line 47)) (0.4.8)\n",
      "Collecting iopath==0.1.9\n",
      "  Using cached iopath-0.1.9-py3-none-any.whl (27 kB)\n",
      "Collecting ipykernel==6.21.1\n",
      "  Using cached ipykernel-6.21.1-py3-none-any.whl (149 kB)\n",
      "Collecting ipython==8.10.0\n",
      "  Using cached ipython-8.10.0-py3-none-any.whl (784 kB)\n",
      "Collecting ipython-genutils==0.2.0\n",
      "  Using cached ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting ipywidgets==8.0.4\n",
      "  Using cached ipywidgets-8.0.4-py3-none-any.whl (137 kB)\n",
      "Collecting isoduration==20.11.0\n",
      "  Using cached isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: jedi==0.18.2 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from -r requirements.txt (line 54)) (0.18.2)\n",
      "Collecting Jinja2==3.1.2\n",
      "  Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "Collecting jsonpointer==2.3\n",
      "  Using cached jsonpointer-2.3-py2.py3-none-any.whl (7.8 kB)\n",
      "Collecting jsonschema==4.17.3\n",
      "  Using cached jsonschema-4.17.3-py3-none-any.whl (90 kB)\n",
      "Collecting jupyter==1.0.0\n",
      "  Using cached jupyter-1.0.0-py2.py3-none-any.whl (2.7 kB)\n",
      "Collecting jupyter-console==6.5.0\n",
      "  Using cached jupyter_console-6.5.0-py3-none-any.whl (23 kB)\n",
      "Collecting jupyter-events==0.6.3\n",
      "  Using cached jupyter_events-0.6.3-py3-none-any.whl (18 kB)\n",
      "Collecting jupyter_client==8.0.2\n",
      "  Using cached jupyter_client-8.0.2-py3-none-any.whl (103 kB)\n",
      "Collecting jupyter_core==5.2.0\n",
      "  Using cached jupyter_core-5.2.0-py3-none-any.whl (94 kB)\n",
      "Collecting jupyter_server==2.2.1\n",
      "  Using cached jupyter_server-2.2.1-py3-none-any.whl (365 kB)\n",
      "Collecting jupyter_server_terminals==0.4.4\n",
      "  Using cached jupyter_server_terminals-0.4.4-py3-none-any.whl (13 kB)\n",
      "Collecting jupyterlab-pygments==0.2.2\n",
      "  Using cached jupyterlab_pygments-0.2.2-py2.py3-none-any.whl (21 kB)\n",
      "Collecting jupyterlab-widgets==3.0.5\n",
      "  Using cached jupyterlab_widgets-3.0.5-py3-none-any.whl (384 kB)\n",
      "Requirement already satisfied: keras==2.11.0 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from -r requirements.txt (line 67)) (2.11.0)\n",
      "Requirement already satisfied: kiwisolver==1.4.4 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from -r requirements.txt (line 68)) (1.4.4)\n",
      "Requirement already satisfied: libclang==15.0.6.1 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from -r requirements.txt (line 69)) (15.0.6.1)\n",
      "Requirement already satisfied: Markdown==3.4.1 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from -r requirements.txt (line 70)) (3.4.1)\n",
      "Requirement already satisfied: MarkupSafe==2.1.2 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from -r requirements.txt (line 71)) (2.1.2)\n",
      "Collecting matplotlib==3.6.3\n",
      "  Using cached matplotlib-3.6.3-cp310-cp310-win_amd64.whl (7.2 MB)\n",
      "Requirement already satisfied: matplotlib-inline==0.1.6 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from -r requirements.txt (line 73)) (0.1.6)\n",
      "Collecting mistune==2.0.5\n",
      "  Using cached mistune-2.0.5-py2.py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: moviepy==1.0.3 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from -r requirements.txt (line 75)) (1.0.3)\n",
      "Collecting mypy-extensions==1.0.0\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Requirement already satisfied: natsort==8.2.0 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from -r requirements.txt (line 77)) (8.2.0)\n",
      "Collecting nbclassic==0.5.1\n",
      "  Using cached nbclassic-0.5.1-py3-none-any.whl (10.0 MB)\n",
      "Collecting nbclient==0.7.2\n",
      "  Using cached nbclient-0.7.2-py3-none-any.whl (71 kB)\n",
      "Collecting nbconvert==7.2.9\n",
      "  Using cached nbconvert-7.2.9-py3-none-any.whl (274 kB)\n",
      "Collecting nbformat==5.7.3\n",
      "  Using cached nbformat-5.7.3-py3-none-any.whl (78 kB)\n",
      "Requirement already satisfied: nest-asyncio==1.5.6 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from -r requirements.txt (line 82)) (1.5.6)\n",
      "Requirement already satisfied: networkx==3.0 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from -r requirements.txt (line 83)) (3.0)\n",
      "Collecting notebook==6.5.2\n",
      "  Using cached notebook-6.5.2-py3-none-any.whl (439 kB)\n",
      "Collecting notebook_shim==0.2.2\n",
      "  Using cached notebook_shim-0.2.2-py3-none-any.whl (13 kB)\n",
      "Collecting numpy==1.24.2\n",
      "  Using cached numpy-1.24.2-cp310-cp310-win_amd64.whl (14.8 MB)\n",
      "Requirement already satisfied: oauthlib==3.2.2 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from -r requirements.txt (line 87)) (3.2.2)\n",
      "Collecting omegaconf==2.3.0\n",
      "  Using cached omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "Requirement already satisfied: opencv-contrib-python==4.7.0.68 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from -r requirements.txt (line 89)) (4.7.0.68)\n",
      "Collecting opencv-python==4.7.0.68\n",
      "  Using cached opencv_python-4.7.0.68-cp37-abi3-win_amd64.whl (38.2 MB)\n",
      "Requirement already satisfied: opt-einsum==3.3.0 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from -r requirements.txt (line 91)) (3.3.0)\n",
      "Collecting packaging==23.0\n",
      "  Using cached packaging-23.0-py3-none-any.whl (42 kB)\n",
      "Collecting pandas==1.5.3\n",
      "  Using cached pandas-1.5.3-cp310-cp310-win_amd64.whl (10.4 MB)\n",
      "Collecting pandocfilters==1.5.0\n",
      "  Using cached pandocfilters-1.5.0-py2.py3-none-any.whl (8.7 kB)\n",
      "Requirement already satisfied: parso==0.8.3 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from -r requirements.txt (line 95)) (0.8.3)\n",
      "Collecting pathspec==0.11.0\n",
      "  Using cached pathspec-0.11.0-py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: pickleshare==0.7.5 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from -r requirements.txt (line 97)) (0.7.5)\n",
      "Collecting Pillow==9.4.0\n",
      "  Using cached Pillow-9.4.0-cp310-cp310-win_amd64.whl (2.5 MB)\n",
      "Collecting platformdirs==3.0.0\n",
      "  Using cached platformdirs-3.0.0-py3-none-any.whl (14 kB)\n",
      "Collecting portalocker==2.7.0\n",
      "  Using cached portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
      "Collecting portpicker==1.2.0\n",
      "  Using cached portpicker-1.2.0.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: proglog==0.1.10 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from -r requirements.txt (line 102)) (0.1.10)\n",
      "Collecting prometheus-client==0.16.0\n",
      "  Using cached prometheus_client-0.16.0-py3-none-any.whl (122 kB)\n",
      "Collecting prompt-toolkit==3.0.36\n",
      "  Using cached prompt_toolkit-3.0.36-py3-none-any.whl (386 kB)\n",
      "Requirement already satisfied: protobuf==3.19.6 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from -r requirements.txt (line 105)) (3.19.6)\n",
      "Requirement already satisfied: psutil==5.9.4 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from -r requirements.txt (line 106)) (5.9.4)\n",
      "Requirement already satisfied: pure-eval==0.2.2 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from -r requirements.txt (line 107)) (0.2.2)\n",
      "Requirement already satisfied: pyasn1==0.4.8 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from -r requirements.txt (line 108)) (0.4.8)\n",
      "Requirement already satisfied: pyasn1-modules==0.2.8 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from -r requirements.txt (line 109)) (0.2.8)\n",
      "Collecting pycocotools==2.0.6\n",
      "  Using cached pycocotools-2.0.6.tar.gz (24 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting pycparser==2.21\n",
      "  Using cached pycparser-2.21-py2.py3-none-any.whl (118 kB)\n",
      "Collecting Pygments==2.14.0\n",
      "  Using cached Pygments-2.14.0-py3-none-any.whl (1.1 MB)\n",
      "Requirement already satisfied: pyparsing==3.0.9 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from -r requirements.txt (line 113)) (3.0.9)\n",
      "Collecting pyrsistent==0.19.3\n",
      "  Using cached pyrsistent-0.19.3-cp310-cp310-win_amd64.whl (62 kB)\n",
      "Requirement already satisfied: python-dateutil==2.8.2 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from -r requirements.txt (line 115)) (2.8.2)\n",
      "Collecting python-json-logger==2.0.4\n",
      "  Using cached python_json_logger-2.0.4-py3-none-any.whl (7.8 kB)\n",
      "Collecting pytz==2022.7.1\n",
      "  Using cached pytz-2022.7.1-py2.py3-none-any.whl (499 kB)\n",
      "Requirement already satisfied: PyWavelets==1.4.1 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from -r requirements.txt (line 118)) (1.4.1)\n",
      "Requirement already satisfied: pywin32==305 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from -r requirements.txt (line 119)) (305)\n",
      "Collecting pywinpty==2.0.10\n",
      "  Using cached pywinpty-2.0.10-cp310-none-win_amd64.whl (1.4 MB)\n",
      "Collecting PyYAML==6.0\n",
      "  Using cached PyYAML-6.0-cp310-cp310-win_amd64.whl (151 kB)\n",
      "Collecting pyzmq==25.0.0\n",
      "  Using cached pyzmq-25.0.0-cp310-cp310-win_amd64.whl (969 kB)\n",
      "Collecting qtconsole==5.4.0\n",
      "  Using cached qtconsole-5.4.0-py3-none-any.whl (121 kB)\n",
      "Collecting QtPy==2.3.0\n",
      "  Using cached QtPy-2.3.0-py3-none-any.whl (83 kB)\n",
      "Collecting requests==2.21.0\n",
      "  Using cached requests-2.21.0-py2.py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: requests-oauthlib==1.3.1 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from -r requirements.txt (line 126)) (1.3.1)\n",
      "Collecting rfc3339-validator==0.1.4\n",
      "  Using cached rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
      "Collecting rfc3986-validator==0.1.1\n",
      "  Using cached rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n",
      "Requirement already satisfied: rsa==4.9 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from -r requirements.txt (line 129)) (4.9)\n",
      "Requirement already satisfied: scikit-image==0.19.3 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from -r requirements.txt (line 130)) (0.19.3)\n",
      "Collecting scipy==1.10.0\n",
      "  Using cached scipy-1.10.0-cp310-cp310-win_amd64.whl (42.5 MB)\n",
      "Collecting Send2Trash==1.8.0\n",
      "  Using cached Send2Trash-1.8.0-py3-none-any.whl (18 kB)\n",
      "Collecting simplegeneric==0.8.1\n",
      "  Using cached simplegeneric-0.8.1.zip (12 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting six==1.12.0\n",
      "  Using cached six-1.12.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting sniffio==1.3.0\n",
      "  Using cached sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
      "Collecting soupsieve==2.3.2.post1\n",
      "  Using cached soupsieve-2.3.2.post1-py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: stack-data==0.6.2 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from -r requirements.txt (line 137)) (0.6.2)\n",
      "Requirement already satisfied: tabulate==0.9.0 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from -r requirements.txt (line 138)) (0.9.0)\n",
      "Requirement already satisfied: tensorboard==2.11.2 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from -r requirements.txt (line 139)) (2.11.2)\n",
      "Requirement already satisfied: tensorboard-data-server==0.6.1 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from -r requirements.txt (line 140)) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit==1.8.1 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from -r requirements.txt (line 141)) (1.8.1)\n",
      "Collecting tensorflow==2.11.0\n",
      "  Using cached tensorflow-2.11.0-cp310-cp310-win_amd64.whl (1.9 kB)\n",
      "Requirement already satisfied: tensorflow-estimator==2.11.0 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from -r requirements.txt (line 143)) (2.11.0)\n",
      "Collecting tensorflow-intel==2.11.0\n",
      "  Using cached tensorflow_intel-2.11.0-cp310-cp310-win_amd64.whl (266.3 MB)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem==0.30.0 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from -r requirements.txt (line 145)) (0.30.0)\n",
      "Requirement already satisfied: termcolor==2.2.0 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from -r requirements.txt (line 146)) (2.2.0)\n",
      "Collecting terminado==0.17.1\n",
      "  Using cached terminado-0.17.1-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: tifffile==2023.2.3 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from -r requirements.txt (line 148)) (2023.2.3)\n",
      "Collecting tinycss2==1.2.1\n",
      "  Using cached tinycss2-1.2.1-py3-none-any.whl (21 kB)\n",
      "Collecting tomli==2.0.1\n",
      "  Using cached tomli-2.0.1-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: torch==1.13.1 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from -r requirements.txt (line 151)) (1.13.1)\n",
      "Requirement already satisfied: torchvision==0.14.1 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from -r requirements.txt (line 152)) (0.14.1)\n",
      "Requirement already satisfied: tornado==6.2 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from -r requirements.txt (line 153)) (6.2)\n",
      "Requirement already satisfied: tqdm==4.64.1 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from -r requirements.txt (line 154)) (4.64.1)\n",
      "Collecting traitlets==5.9.0\n",
      "  Using cached traitlets-5.9.0-py3-none-any.whl (117 kB)\n",
      "Collecting typing_extensions==4.4.0\n",
      "  Using cached typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
      "Collecting uri-template==1.2.0\n",
      "  Using cached uri_template-1.2.0-py3-none-any.whl (10 kB)\n",
      "Collecting urllib3==1.24.3\n",
      "  Using cached urllib3-1.24.3-py2.py3-none-any.whl (118 kB)\n",
      "Collecting wcwidth==0.2.6\n",
      "  Using cached wcwidth-0.2.6-py2.py3-none-any.whl (29 kB)\n",
      "Collecting webcolors==1.12\n",
      "  Using cached webcolors-1.12-py3-none-any.whl (9.9 kB)\n",
      "Collecting webencodings==0.5.1\n",
      "  Using cached webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Collecting websocket-client==1.5.1\n",
      "  Using cached websocket_client-1.5.1-py3-none-any.whl (55 kB)\n",
      "Collecting Werkzeug==2.2.2\n",
      "  Using cached Werkzeug-2.2.2-py3-none-any.whl (232 kB)\n",
      "Collecting widgetsnbextension==4.0.5\n",
      "  Using cached widgetsnbextension-4.0.5-py3-none-any.whl (2.0 MB)\n",
      "Requirement already satisfied: wrapt==1.14.1 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from -r requirements.txt (line 165)) (1.14.1)\n",
      "Collecting yacs==0.1.8\n",
      "  Using cached yacs-0.1.8-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from astunparse==1.6.3->-r requirements.txt (line 8)) (0.38.4)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\program files\\windowsapps\\pythonsoftwarefoundation.python.3.10_3.10.2800.0_x64__qbz5n2kfra8p0\\lib\\site-packages (from tensorboard==2.11.2->-r requirements.txt (line 139)) (65.5.0)\n",
      "INFO: pip is looking at multiple versions of tabulate to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting tabulate==0.9.0\n",
      "  Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "INFO: pip is looking at multiple versions of stack-data to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting stack-data==0.6.2\n",
      "  Using cached stack_data-0.6.2-py3-none-any.whl (24 kB)\n",
      "INFO: pip is looking at multiple versions of soupsieve to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of sniffio to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of six to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of simplegeneric to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of send2trash to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of scipy to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of scikit-image to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting scikit-image==0.19.3\n",
      "  Using cached scikit_image-0.19.3-cp310-cp310-win_amd64.whl (12.0 MB)\n",
      "INFO: pip is looking at multiple versions of rsa to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting rsa==4.9\n",
      "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "INFO: pip is looking at multiple versions of rfc3986-validator to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of rfc3339-validator to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of requests-oauthlib to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting requests-oauthlib==1.3.1\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "INFO: pip is looking at multiple versions of requests to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of qtpy to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of qtconsole to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of pyzmq to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of pyyaml to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of pywinpty to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of pywin32 to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pywin32==305\n",
      "  Using cached pywin32-305-cp310-cp310-win_amd64.whl (12.1 MB)\n",
      "INFO: pip is looking at multiple versions of pywavelets to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting PyWavelets==1.4.1\n",
      "  Using cached PyWavelets-1.4.1-cp310-cp310-win_amd64.whl (4.2 MB)\n",
      "INFO: pip is looking at multiple versions of pytz to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of python-json-logger to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of python-dateutil to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting python-dateutil==2.8.2\n",
      "  Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "INFO: pip is looking at multiple versions of pyrsistent to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of pyparsing to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pyparsing==3.0.9\n",
      "  Using cached pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
      "INFO: pip is looking at multiple versions of pygments to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of pycparser to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of pycocotools to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of pyasn1-modules to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pyasn1-modules==0.2.8\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "INFO: pip is looking at multiple versions of pyasn1 to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pyasn1==0.4.8\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "INFO: pip is looking at multiple versions of pure-eval to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pure-eval==0.2.2\n",
      "  Using cached pure_eval-0.2.2-py3-none-any.whl (11 kB)\n",
      "INFO: pip is looking at multiple versions of psutil to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting psutil==5.9.4\n",
      "  Using cached psutil-5.9.4-cp36-abi3-win_amd64.whl (252 kB)\n",
      "INFO: pip is looking at multiple versions of protobuf to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting protobuf==3.19.6\n",
      "  Using cached protobuf-3.19.6-cp310-cp310-win_amd64.whl (895 kB)\n",
      "INFO: pip is looking at multiple versions of prompt-toolkit to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of prometheus-client to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of proglog to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting proglog==0.1.10\n",
      "  Using cached proglog-0.1.10-py3-none-any.whl (6.1 kB)\n",
      "INFO: pip is looking at multiple versions of portpicker to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of portalocker to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of platformdirs to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of pillow to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of pickleshare to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pickleshare==0.7.5\n",
      "  Using cached pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB)\n",
      "INFO: pip is looking at multiple versions of pathspec to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of parso to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting parso==0.8.3\n",
      "  Using cached parso-0.8.3-py2.py3-none-any.whl (100 kB)\n",
      "INFO: pip is looking at multiple versions of pandocfilters to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of pandas to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of packaging to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of opt-einsum to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting opt-einsum==3.3.0\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "INFO: pip is looking at multiple versions of opencv-python to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of opencv-contrib-python to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting opencv-contrib-python==4.7.0.68\n",
      "  Using cached opencv_contrib_python-4.7.0.68-cp37-abi3-win_amd64.whl (44.9 MB)\n",
      "INFO: pip is looking at multiple versions of omegaconf to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of oauthlib to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting oauthlib==3.2.2\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "INFO: pip is looking at multiple versions of numpy to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of notebook-shim to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of notebook to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of networkx to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting networkx==3.0\n",
      "  Using cached networkx-3.0-py3-none-any.whl (2.0 MB)\n",
      "INFO: pip is looking at multiple versions of nest-asyncio to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting nest-asyncio==1.5.6\n",
      "  Using cached nest_asyncio-1.5.6-py3-none-any.whl (5.2 kB)\n",
      "INFO: pip is looking at multiple versions of nbformat to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of nbconvert to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of nbclient to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of nbclassic to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of natsort to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting natsort==8.2.0\n",
      "  Using cached natsort-8.2.0-py3-none-any.whl (37 kB)\n",
      "INFO: pip is looking at multiple versions of mypy-extensions to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of moviepy to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting moviepy==1.0.3\n",
      "  Using cached moviepy-1.0.3.tar.gz (388 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "INFO: pip is looking at multiple versions of mistune to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of matplotlib-inline to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting matplotlib-inline==0.1.6\n",
      "  Using cached matplotlib_inline-0.1.6-py3-none-any.whl (9.4 kB)\n",
      "INFO: pip is looking at multiple versions of matplotlib to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of markupsafe to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting MarkupSafe==2.1.2\n",
      "  Using cached MarkupSafe-2.1.2-cp310-cp310-win_amd64.whl (16 kB)\n",
      "INFO: pip is looking at multiple versions of markdown to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting Markdown==3.4.1\n",
      "  Using cached Markdown-3.4.1-py3-none-any.whl (93 kB)\n",
      "INFO: pip is looking at multiple versions of libclang to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting libclang==15.0.6.1\n",
      "  Using cached libclang-15.0.6.1-py2.py3-none-win_amd64.whl (23.2 MB)\n",
      "INFO: pip is looking at multiple versions of kiwisolver to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting kiwisolver==1.4.4\n",
      "  Using cached kiwisolver-1.4.4-cp310-cp310-win_amd64.whl (55 kB)\n",
      "INFO: pip is looking at multiple versions of keras to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting keras==2.11.0\n",
      "  Using cached keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
      "INFO: pip is looking at multiple versions of jupyterlab-widgets to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of jupyterlab-pygments to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of jupyter-server-terminals to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of jupyter-server to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of jupyter-core to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of jupyter-client to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of jupyter-events to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of jupyter-console to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of jupyter to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of jsonschema to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of jsonpointer to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of jinja2 to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of jedi to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting jedi==0.18.2\n",
      "  Using cached jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
      "INFO: pip is looking at multiple versions of isoduration to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of ipywidgets to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of ipython-genutils to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of ipython to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of ipykernel to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of iopath to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of imageio-ffmpeg to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting imageio-ffmpeg==0.4.8\n",
      "  Using cached imageio_ffmpeg-0.4.8-py3-none-win_amd64.whl (22.6 MB)\n",
      "INFO: pip is looking at multiple versions of imageio to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of idna to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of hydra-core to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of h5py to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting h5py==3.8.0\n",
      "  Using cached h5py-3.8.0-cp310-cp310-win_amd64.whl (2.6 MB)\n",
      "INFO: pip is looking at multiple versions of grpcio to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting grpcio==1.51.1\n",
      "  Using cached grpcio-1.51.1-cp310-cp310-win_amd64.whl (3.7 MB)\n",
      "INFO: pip is looking at multiple versions of google-pasta to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting google-pasta==0.2.0\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "INFO: pip is looking at multiple versions of google-auth-oauthlib to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting google-auth-oauthlib==0.4.6\n",
      "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "INFO: pip is looking at multiple versions of google-auth to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of gast to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting gast==0.4.0\n",
      "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "INFO: pip is looking at multiple versions of fvcore to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of fqdn to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of fonttools to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting fonttools==4.38.0\n",
      "  Using cached fonttools-4.38.0-py3-none-any.whl (965 kB)\n",
      "INFO: pip is looking at multiple versions of flatbuffers to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting flatbuffers==23.1.21\n",
      "  Using cached flatbuffers-23.1.21-py2.py3-none-any.whl (26 kB)\n",
      "INFO: pip is looking at multiple versions of google-auth-oauthlib to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of fer to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting fer==22.5.0\n",
      "  Using cached fer-22.5.0-py3-none-any.whl (1.5 MB)\n",
      "INFO: pip is looking at multiple versions of fastjsonschema to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of facenet-pytorch to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting facenet-pytorch==2.5.2\n",
      "  Using cached facenet_pytorch-2.5.2-py3-none-any.whl (1.9 MB)\n",
      "INFO: pip is looking at multiple versions of executing to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting executing==1.2.0\n",
      "  Using cached executing-1.2.0-py2.py3-none-any.whl (24 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "INFO: pip is looking at multiple versions of entrypoints to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting entrypoints==0.4\n",
      "  Using cached entrypoints-0.4-py3-none-any.whl (5.3 kB)\n",
      "INFO: pip is looking at multiple versions of google-auth to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of defusedxml to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of decorator to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting decorator==4.4.2\n",
      "  Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB)\n",
      "INFO: pip is looking at multiple versions of debugpy to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of cycler to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting cycler==0.11.0\n",
      "  Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "INFO: pip is looking at multiple versions of contourpy to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of comm to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of colorthief to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting colorthief==0.2.1\n",
      "  Using cached colorthief-0.2.1-py2.py3-none-any.whl (6.1 kB)\n",
      "INFO: pip is looking at multiple versions of colorama to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting colorama==0.4.6\n",
      "  Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "INFO: pip is looking at multiple versions of cloudpickle to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of click to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of charset-normalizer to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of chardet to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of cffi to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of certifi to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of cachetools to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting cachetools==5.3.0\n",
      "  Using cached cachetools-5.3.0-py3-none-any.whl (9.3 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "INFO: pip is looking at multiple versions of bleach to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of black to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of beautifulsoup4 to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of backcall to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting backcall==0.2.0\n",
      "  Using cached backcall-0.2.0-py2.py3-none-any.whl (11 kB)\n",
      "INFO: pip is looking at multiple versions of attrs to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of astunparse to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting astunparse==1.6.3\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "INFO: pip is looking at multiple versions of asttokens to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting asttokens==2.2.1\n",
      "  Using cached asttokens-2.2.1-py2.py3-none-any.whl (26 kB)\n",
      "INFO: pip is looking at multiple versions of arrow to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of argon2-cffi-bindings to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of argon2-cffi to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of <Python from Requires-Python> to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of anyio to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of antlr4-python3-runtime to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of absl-py to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting absl-py==1.4.0\n",
      "  Using cached absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "\n",
      "The conflict is caused by:\n",
      "    The user requested google-auth==1.4.2\n",
      "    google-auth-oauthlib 0.4.6 depends on google-auth>=1.0.0\n",
      "    tensorboard 2.11.2 depends on google-auth<3 and >=1.6.3\n",
      "\n",
      "To fix this you could try to:\n",
      "1. loosen the range of package versions you've specified\n",
      "2. remove package versions to allow pip attempt to solve the dependency conflict\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Cannot install -r requirements.txt (line 40), google-auth==1.4.2 and tensorboard==2.11.2 because these package versions have conflicting dependencies.\n",
      "ERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install -r requirements.txt\n",
    "#!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Imports\n",
    "- uncomment detectron2 and related imports if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, json, os, math, pathlib, random, re, sys, statistics, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import torch, torchvision\n",
    "import pylab\n",
    "#import detectron2\n",
    "\n",
    "from natsort import natsorted, ns\n",
    "from colorthief import ColorThief\n",
    "from fer import FER\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.io as io #scikit-image\n",
    "\n",
    "from torch.autograd import Variable as V\n",
    "from torchvision import transforms as trn\n",
    "from torch.nn import functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "#from detectron2.utils.logger import setup_logger\n",
    "#from detectron2 import model_zoo\n",
    "#from detectron2.engine import DefaultPredictor\n",
    "#from detectron2.config import get_cfg\n",
    "#from detectron2.utils.visualizer import Visualizer, ColorMode\n",
    "#from detectron2.data import MetadataCatalog, DatasetCatalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup logger for detectron2\n",
    "#setup_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Configuration:\n",
    "\n",
    "  def __init__(self):\n",
    "    with open(\"./config.json\", \"rb\") as file:\n",
    "      self.config = json.load(file)\n",
    "    \n",
    "    self.inputRoot = pathlib.Path(self.config[\"inputRoot\"])\n",
    "    self.outputRoot = pathlib.Path(self.config[\"outputRoot\"])\n",
    "\n",
    "  def getInputRoot(self):\n",
    "    return self.config[\"inputRoot\"] + \"\\\\\"\n",
    "\n",
    "  def getOutputRoot(self):\n",
    "    return self.config[\"outputRoot\"] + \"\\\\\"\n",
    "\n",
    "  def getGameNames(self):\n",
    "    return [gameName for gameName in self.config[\"games\"]]\n",
    "\n",
    "  def getMethodNames(self):\n",
    "    return self.config[\"methods\"]\n",
    "\n",
    "  def checkInputFolder(self):\n",
    "    \"\"\"\n",
    "      Checks if the base input path has directories for each game. Also checks if each directory has game images, otherwise prints warnings.\n",
    "      Not checking might result in an error because there is no folder to write to. This methods raises expections if folders are missing\n",
    "      and warns if there are folders or files that are not defined in config.json.\n",
    "    \"\"\"\n",
    "    gameNames = self.getGameNames()\n",
    "    \n",
    "    for gameFolder in self.inputRoot.iterdir():\n",
    "      if gameFolder.is_dir() and gameFolder.name in gameNames:\n",
    "        gameNames.remove(gameFolder.name)\n",
    "\n",
    "        if not any(gameFolder.iterdir()):\n",
    "          print(f\"WARNING: {gameFolder} contains no game images\")\n",
    "      else:\n",
    "        print(f\"WARNING: {gameFolder} not in games or not a directory\")\n",
    "\n",
    "    if len(gameNames) != 0:\n",
    "      raise Exception(f\"there are the following missing folders: {', '.join(gameNames)}\")\n",
    "\n",
    "    print(\"Input folder is ok.\")\n",
    "  \n",
    "  def checkOutputFolder(self):\n",
    "    \"\"\"\n",
    "      Checks if the base output path has directories for each CV method. Also checks if each directory has subfolders for each game.\n",
    "      Not checking might result in an error because there is no folder to write to. This methods raises expections if folders are missing\n",
    "      and warns if there are folders or files that are not defined in config.json. Prints warning if folders contain files that might\n",
    "      be overwritten.\n",
    "    \"\"\"\n",
    "    methodNames = self.getMethodNames()\n",
    "    \n",
    "    for methodFolder in self.outputRoot.iterdir():\n",
    "      if methodFolder.is_dir() and methodFolder.name in methodNames:\n",
    "        methodNames.remove(methodFolder.name)\n",
    "\n",
    "        games = pathlib.Path(methodFolder)\n",
    "        gameNames = self.getGameNames()\n",
    "\n",
    "        for gameFolder in games.iterdir():\n",
    "          if gameFolder.is_dir() and gameFolder.name in gameNames:\n",
    "            gameNames.remove(gameFolder.name)\n",
    "\n",
    "            if any(gameFolder.iterdir()):\n",
    "              print(f\"WARNING: {gameFolder} contains files\")  \n",
    "          else:\n",
    "            print(f\"WARNING: {gameFolder} not in games or not a directory\")\n",
    "          pass\n",
    "\n",
    "        if len(gameNames) != 0:\n",
    "          raise Exception(f\"there are the following missing folders in {methodFolder}: {', '.join(gameNames)}\")\n",
    "\n",
    "      else:\n",
    "        print(f\"WARNING: {methodFolder} not in methods or not a directory\")\n",
    "\n",
    "    if len(methodNames) != 0:\n",
    "      raise Exception(f\"there are the following missing folders: {', '.join(methodNames)}\")\n",
    "\n",
    "    print(\"Output folder is ok.\")\n",
    "\n",
    "  def getFileLength(self):\n",
    "    numGames = 0\n",
    "    gameNames = self.getGameNames()\n",
    "    \n",
    "    for gameFolder in self.inputRoot.iterdir():\n",
    "      if gameFolder.is_dir() and gameFolder.name in gameNames:\n",
    "        numGames += len(list(gameFolder.iterdir()))\n",
    "    \n",
    "    return numGames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgeGender:\n",
    "\n",
    "  def __init__(self, gameName, outputPath):\n",
    "    self.method = \"AgeGender\"\n",
    "    self.gameName = gameName\n",
    "    self.outputPathCSV = f\"{outputPath}{self.method}\\\\\"\n",
    "    self.outputPathImages = f\"{self.outputPathCSV}\\\\{self.gameName}\\\\\"\n",
    "  \n",
    "    # Defined the model files\n",
    "    self.FACE_PROTO = \"./models/opencv_face_detector.pbtxt\"\n",
    "    self.FACE_MODEL = \"./models/opencv_face_detector_uint8.pb\"\n",
    "    self.AGE_PROTO = \"./models/age_deploy.prototxt\"\n",
    "    self.AGE_MODEL = \"./models/age_net.caffemodel\"\n",
    "    self.GENDER_PROTO = \"./models/gender_deploy.prototxt\"\n",
    "    self.GENDER_MODEL = \"./models/gender_net.caffemodel\"\n",
    "\n",
    "    # Load network\n",
    "    self.FACE_NET = cv2.dnn.readNet(self.FACE_MODEL, self.FACE_PROTO)\n",
    "    self.AGE_NET = cv2.dnn.readNet(self.AGE_MODEL, self.AGE_PROTO)\n",
    "    self.GENDER_NET = cv2.dnn.readNet(self.GENDER_MODEL, self.GENDER_PROTO)\n",
    "\n",
    "    self.MODEL_MEAN_VALUES = (78.4263377603, 87.7689143744, 114.895847746)\n",
    "    self.AGE_LIST = [\"(0-2)\", \"(4-6)\", \"(8-12)\", \"(15-20)\", \"(25-32)\", \"(38-43)\", \"(48-53)\", \"(60-100)\"]\n",
    "    self.GENDER_LIST = [\"Male\", \"Female\"]\n",
    "\n",
    "    self.box_padding = 20\n",
    "\n",
    "    self.frameName = []\n",
    "    self.person = []\n",
    "    self.boxFace = []\n",
    "    self.gen = []\n",
    "    self.gender_conf = []\n",
    "    self.ages = []\n",
    "    self.age_conf = []\n",
    "\n",
    "  def get_face_box (self, net, frame, conf_threshold = 0.5):\n",
    "    frame_copy = frame.copy()\n",
    "    frame_height = frame_copy.shape[0]\n",
    "    frame_width = frame_copy.shape[1]\n",
    "    blob = cv2.dnn.blobFromImage(frame_copy, 1.0, (300, 300), [104, 117, 123], True, False)\n",
    "\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward()\n",
    "    boxes = []\n",
    "\n",
    "    for i in range(detections.shape[2]):\n",
    "      confidence = detections[0, 0, i, 2]\n",
    "\n",
    "      if confidence > conf_threshold:\n",
    "        x1 = int(detections[0, 0, i, 3] * frame_width)\n",
    "        y1 = int(detections[0, 0, i, 4] * frame_height)\n",
    "        x2 = int(detections[0, 0, i, 5] * frame_width)\n",
    "        y2 = int(detections[0, 0, i, 6] * frame_height)\n",
    "        boxes.append([x1, y1, x2, y2])\n",
    "        cv2.rectangle(frame_copy, (x1, y1), (x2, y2), (0, 255, 0), int(round(frame_height / 150)), 8)\n",
    "\n",
    "    return frame_copy, boxes\n",
    "\n",
    "  def handleNextImage(self, fileName, image):\n",
    "    resized_image = cv2.resize(image, (640, 480))\n",
    "\n",
    "    frame = resized_image.copy()\n",
    "    frame_face, boxes = self.get_face_box(self.FACE_NET, frame)\n",
    "\n",
    "    count = 0\n",
    "    for box in boxes:\n",
    "      self.frameName.append(fileName)\n",
    "      self.person.append(count)\n",
    "      self.boxFace.append(box)\n",
    "      face = frame[max(0, box[1] - self.box_padding):min(box[3] + self.box_padding, frame.shape[0] - 1), \\\n",
    "        max(0, box[0] - self.box_padding):min(box[2] + self.box_padding, frame.shape[1] - 1)]\n",
    "\n",
    "      blob = cv2.dnn.blobFromImage(face, 1.0, (227, 227), self.MODEL_MEAN_VALUES, swapRB = False)\n",
    "      self.GENDER_NET.setInput(blob)\n",
    "      gender_predictions = self.GENDER_NET.forward()\n",
    "      gender = self.GENDER_LIST[gender_predictions[0].argmax()]\n",
    "      self.gen.append(gender)\n",
    "      self.gender_conf.append(gender_predictions[0].max())\n",
    "\n",
    "      self.AGE_NET.setInput(blob)\n",
    "      age_predictions = self.AGE_NET.forward()\n",
    "      age = self.AGE_LIST[age_predictions[0].argmax()]\n",
    "      self.ages.append(age)\n",
    "      self.age_conf.append(age_predictions[0].max())\n",
    "\n",
    "      label = \"{},{}\".format(gender, age)\n",
    "      cv2.putText(frame_face, label, (box[0], box[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2, cv2.LINE_AA)\n",
    "      cv2.putText(frame_face, str(count), (box[0] + 2, box[1] + 15), cv2.FONT_HERSHEY_SIMPLEX,0.5,(0,255,0),1,cv2.LINE_AA, )\n",
    "\n",
    "      count += 1\n",
    "\n",
    "    #only saves an image if age/gender was found\n",
    "    if len(boxes) > 0:\n",
    "      cv2.resize(frame_face, (640, 480))\n",
    "      cv2.imwrite(f\"{self.outputPathImages}{fileName}\", frame_face)\n",
    "\n",
    "  def dataToCSV(self):\n",
    "    if len(self.frameName) > 0:\n",
    "      df = pd.DataFrame(list(zip(self.frameName, self.person, self.boxFace, self.gen, self.gender_conf, self.ages, self.age_conf)))\n",
    "      df.columns = ['Name', 'person', 'box', 'gender', 'gender_conf', 'age', 'age_conf']\n",
    "      df.to_csv(f\"{self.outputPathCSV}{self.gameName}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrightnessAnalyzer:\n",
    "\n",
    "  def __init__(self, gameName, outputPath):\n",
    "    self.method = \"Brightness\"\n",
    "    self.gameName = gameName\n",
    "    self.outputPathCSV = f\"{outputPath}{self.method}\\\\\"\n",
    "    \n",
    "    self.brightness_dict = {}\n",
    "    self.brightness_values_dict = {}\n",
    "    self.statistic_result_dict = {}\n",
    "\n",
    "  def handleNextImage(self, fileName, image):\n",
    "    greyscale_img = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    img_brightness = greyscale_img.mean()/255\n",
    "\n",
    "    self.brightness_values_dict[fileName] = img_brightness\n",
    "\n",
    "    self.brightness_dict[fileName] = []\n",
    "    self.brightness_dict[fileName].append(round(img_brightness, 2))\n",
    "    self.brightness_dict[fileName].append(fileName)\n",
    "\n",
    "  def dataToCSV(self):\n",
    "    mean_brightness = sum(self.brightness_values_dict.values()) / len(self.brightness_dict)\n",
    "    sd_brightness = statistics.stdev(self.brightness_values_dict.values())\n",
    "    median_brightness = statistics.stdev(self.brightness_values_dict.values())\n",
    "    min_brightness = min(self.brightness_values_dict.values())\n",
    "    max_brightness = max(self.brightness_values_dict.values())\n",
    "\n",
    "    statistic_result_dict = {}\n",
    "    statistic_result_dict[\"min\"] = []\n",
    "    statistic_result_dict[\"min\"].append(round(min_brightness, 2))\n",
    "    statistic_result_dict[\"median\"] = []\n",
    "    statistic_result_dict[\"median\"].append(round(median_brightness, 2))\n",
    "    statistic_result_dict[\"mean\"] = []\n",
    "    statistic_result_dict[\"mean\"].append(round(mean_brightness, 2))\n",
    "    statistic_result_dict[\"sd\"] = []\n",
    "    statistic_result_dict[\"sd\"].append(round(sd_brightness, 2))\n",
    "    statistic_result_dict[\"max\"] = []\n",
    "    statistic_result_dict[\"max\"].append(round(max_brightness, 2))\n",
    "\n",
    "    dataBrightness = pd.DataFrame.from_dict(self.brightness_dict, orient='index', columns=[\"brightness_value\", \"frame\"])\n",
    "    dataResults = pd.DataFrame.from_dict(statistic_result_dict)\n",
    "\n",
    "    cols = dataBrightness.columns.tolist()\n",
    "    cols = cols[-1:] + cols[:-1]\n",
    "    dataBrightness = dataBrightness[cols]\n",
    "\n",
    "    dataBrightness.to_csv(f\"{self.outputPathCSV}{self.gameName}_Brightness.csv\", encoding=\"utf-8\", index=False)\n",
    "\n",
    "    cols = dataResults.columns.tolist()\n",
    "    cols = cols[-1:] + cols[:-1]\n",
    "    dataResults = dataResults[cols]\n",
    "    \n",
    "    dataResults.to_csv(f\"{self.outputPathCSV}{self.gameName}_Statistics.csv\", encoding=\"utf-8\", index=False)\n",
    "\n",
    "  def generateGreyScaleGraph(self):\n",
    "    brightness_values_list = self.brightness_values_dict.items()\n",
    "\n",
    "    x, y = zip(*brightness_values_list)\n",
    "\n",
    "    plt.plot(y, color=\"navy\")\n",
    "    plt.xlabel(\"frame\")\n",
    "    plt.ylabel(\"greyscale value\")\n",
    "\n",
    "    #plt.title(\"Grayscale values for \" + self.gameName , fontsize= 15, pad=20)\n",
    "    #plt.legend([\"greyscale value 0 = complete black image\\ngreyscale value 1 = complete white image\"])\n",
    "\n",
    "    plt.savefig(f\"{self.outputPathCSV}{self.gameName}_Graph.svg\", format='svg')\n",
    "\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomColorThief(ColorThief):\n",
    "  def __init__(self, image):\n",
    "    self.image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "class DominantColor:\n",
    "  def __init__(self, gameName, outputPath):\n",
    "    self.method = \"DominantColor\"\n",
    "    self.gameName = gameName\n",
    "    self.outputPathCSV = f\"{outputPath}{self.method}\\\\\"\n",
    "\n",
    "    self.dominant_color_arr = []\n",
    "    self.frameNames = []\n",
    "\n",
    "  def handleNextImage(self, frameName, image):\n",
    "    ct_frame = CustomColorThief(image)\n",
    "    # change quality to \"2\" to get the second most dominat color\n",
    "    dominant_color_of_ct_frame = ct_frame.get_color(quality=1)\n",
    "    self.dominant_color_arr.append(dominant_color_of_ct_frame)\n",
    "    self.frameNames.append(frameName)\n",
    "\n",
    "  def generateColorBars(self):\n",
    "    plt.ioff()\n",
    "    plt.imshow([[self.dominant_color_arr[i] for i in range(len(self.dominant_color_arr))]], extent=[0,len(self.dominant_color_arr),0,1], aspect='auto')\n",
    "    plt.ion()\n",
    "\n",
    "    # axis adjustments\n",
    "    plt.xlabel(\"frame\")\n",
    "    plt.ylabel(\"dominant color\")\n",
    "    plt.yticks([])\n",
    "\n",
    "    # enable the line if you want your plot to have a title\n",
    "    #plt.title(\"DominantColorBarcode for \" + self.gameName, fontsize= 15, pad=20)\n",
    "\n",
    "    #plt.savefig(f\"{self.outputPathCSV}{self.gameName}_dominant_color_for_each_frame.eps\", format='eps')\n",
    "    plt.savefig(f\"{self.outputPathCSV}{self.gameName}_dominant_color_for_each_frame.svg\", format='svg')\n",
    "    #plt.savefig(f\"{self.outputPathCSV}{self.gameName}_dominant_color_for_each_frame.png\")\n",
    "    #plt.savefig(f\"{self.outputPathCSV}{self.gameName}_dominant_color_for_each_frame.jpg\")\n",
    "\n",
    "    plt.close()\n",
    "  \n",
    "  def dataToCSV(self):\n",
    "    dominant_color_dict = {}\n",
    "\n",
    "    for idx, frame_name in enumerate(self.dominant_color_arr):\n",
    "      dominant_color_dict[frame_name] = []\n",
    "      dominant_color_dict[frame_name].append(self.dominant_color_arr[idx])\n",
    "      dominant_color_dict[frame_name].append(self.frameNames[idx])\n",
    "\n",
    "    dominant_color_row_data = pd.DataFrame.from_dict(dominant_color_dict, orient='index', columns=[\"RGB\", \"frame\"])\n",
    "    \n",
    "    cols_row_data = dominant_color_row_data.columns.tolist()\n",
    "    cols_row_data = cols_row_data[-1:] + cols_row_data[:-1]\n",
    "    dominant_color_row_data = dominant_color_row_data[cols_row_data]\n",
    "    \n",
    "    dominant_color_row_data.to_csv(f\"{self.outputPathCSV}{self.gameName}_dominant_color_row_data.csv\", encoding=\"utf-8\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageColor:\n",
    "\n",
    "  def __init__(self, gameName, outputPath):\n",
    "    self.method = \"AverageColor\"\n",
    "    self.gameName = gameName\n",
    "    self.outputPathCSV = f\"{outputPath}{self.method}\\\\\"\n",
    "\n",
    "    self.averageColorArr = []\n",
    "    self.frameNames = []\n",
    "\n",
    "  def handleNextImage(self, frameName, image):\n",
    "    average_color_row = np.average(image, axis=0)\n",
    "    average_color = np.average(average_color_row, axis=0)\n",
    "\n",
    "    average_color_tuple = tuple(average_color)\n",
    "\n",
    "    self.averageColorArr.append(average_color_tuple)\n",
    "    self.frameNames.append(frameName)\n",
    "\n",
    "  def generateColorBars(self):\n",
    "    self.averageColorArr = tuple(tuple(map(int, tup)) for tup in self.averageColorArr)\n",
    "    \n",
    "    plt.ioff()\n",
    "    plt.imshow([[self.averageColorArr[i] for i in range(len(self.averageColorArr))]], extent=[0,len(self.averageColorArr),0,1], aspect='auto')\n",
    "    plt.ioff()\n",
    "\n",
    "    # axis adjustments\n",
    "    plt.xlabel(\"frame\")\n",
    "    plt.ylabel(\"average color\")\n",
    "    plt.yticks([])\n",
    "\n",
    "    # enable the line if you want your plot to have a title\n",
    "    #plt.title(\"AverageColorBarcode for \" + self.gameName, fontsize= 15, pad=20)\n",
    "\n",
    "    #plt.savefig(f\"{self.outputPathCSV}{self.gameName}_average_color_for_each_frame.eps\", format='eps')\n",
    "    plt.savefig(f\"{self.outputPathCSV}{self.gameName}_average_color_for_each_frame.svg\", format='svg')\n",
    "    #plt.savefig(f\"{self.outputPathCSV}{self.gameName}_average_color_for_each_frame.png\")\n",
    "    #plt.savefig(f\"{self.outputPathCSV}{self.gameName}_average_color_for_each_frame.jpg\")\n",
    "\n",
    "    plt.close()\n",
    "\n",
    "  def dataToCSV(self):\n",
    "    average_color_dict = {}\n",
    "\n",
    "    for idx, frame_name in enumerate(self.averageColorArr):\n",
    "      average_color_dict[frame_name] = []\n",
    "      average_color_dict[frame_name].append(self.averageColorArr[idx])\n",
    "      average_color_dict[frame_name].append(self.frameNames[idx])\n",
    "\n",
    "    average_color_row_data = pd.DataFrame.from_dict(average_color_dict, orient='index', columns=[\"RGB\", \"frame\"])\n",
    "    \n",
    "    cols_row_data = average_color_row_data.columns.tolist()\n",
    "    cols_row_data = cols_row_data[-1:] + cols_row_data[:-1]\n",
    "    average_color_row_data = average_color_row_data[cols_row_data]\n",
    "    \n",
    "    average_color_row_data.to_csv(f\"{self.outputPathCSV}{self.gameName}_average_color_row_data.csv\", encoding=\"utf-8\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionDetection:\n",
    "\n",
    "  def __init__(self, gameName, outputPath):\n",
    "    self.method = \"Emotion\"\n",
    "    self.gameName = gameName\n",
    "    self.outputPathCSV = f\"{outputPath}{self.method}\\\\\"\n",
    "    self.outputPathImages = f\"{self.outputPathCSV}\\\\{self.gameName}\\\\\"\n",
    "\n",
    "    self.detectorMT = FER(mtcnn=True)\n",
    "\n",
    "    self.emotions = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']\n",
    "    self.categories = ['box', 'emotions']\n",
    "\n",
    "    self.frame = []\n",
    "    self.person = []\n",
    "    self.box = []\n",
    "    self.angry = []\n",
    "    self.disgust = []\n",
    "    self.fear = []\n",
    "    self.happy = []\n",
    "    self.sad = []\n",
    "    self.suprise = []\n",
    "    self.neutral = []\n",
    "\n",
    "  def handleNextImage(self, frameName, frameData):\n",
    "    prediction = self.detectorMT.detect_emotions(frameData)\n",
    "\n",
    "    for n in range(len(prediction)):\n",
    "      bbox = prediction[n][self.categories[0]]\n",
    "      img = cv2.rectangle(frameData, (bbox[0], bbox[1]), (bbox[0] + bbox[2], bbox[1] + bbox[3]),(0, 255, 0), 1,)\n",
    "      cv2.putText(img, str(n), (bbox[0] + 2, bbox[1] + 15), cv2.FONT_HERSHEY_SIMPLEX,0.5,(0,255,0),1,cv2.LINE_AA, )\n",
    "      self.frame.append(frameName)\n",
    "      self.person.append(n)\n",
    "      self.box.append(bbox)\n",
    "      self.angry.append(prediction[n][self.categories[1]][self.emotions[0]])\n",
    "      self.disgust.append(prediction[n][self.categories[1]][self.emotions[1]])\n",
    "      self.fear.append(prediction[n][self.categories[1]][self.emotions[2]])\n",
    "      self.happy.append(prediction[n][self.categories[1]][self.emotions[3]])\n",
    "      self.sad.append(prediction[n][self.categories[1]][self.emotions[4]])\n",
    "      self.suprise.append(prediction[n][self.categories[1]][self.emotions[5]])\n",
    "      self.neutral.append(prediction[n][self.categories[1]][self.emotions[6]])\n",
    "\n",
    "    if len(prediction) > 0:\n",
    "      cv2.imwrite(f\"{self.outputPathImages}{frameName}\", img)\n",
    "\n",
    "  def dataToCSV(self):\n",
    "    df = pd.DataFrame(list(zip(self.frame, self.person, self.box, self.angry, self.disgust, self.fear, self.happy, self.sad, self.suprise, self.neutral)))\n",
    "    df.columns = ['Name', 'person', 'box', 'angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']\n",
    "    df.to_csv(f\"{self.outputPathCSV}{self.gameName}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocationDetection:\n",
    "\n",
    "  def __init__(self, gameName, outputPath):\n",
    "    self.method = \"Location\"\n",
    "    self.gameName = gameName\n",
    "    self.outputPathCSV = f\"{outputPath}{self.method}\\\\\"\n",
    "    self.outputPathImages = f\"{self.outputPathCSV}\\\\{self.gameName}\\\\\"\n",
    "\n",
    "    self.CATEGORIES_PLACES = \"./models/categories_places365.txt\"\n",
    "    self.IO_PLACES = \"./models/IO_places365.txt\"\n",
    "    self.LABELS_SUNATTRIBUTE = \"./models/labels_sunattribute.txt\"\n",
    "    self.SCENE_ATTRIBUTE_NPY = \"./models/W_sceneattribute_wideresnet18.npy\"\n",
    "\n",
    "    self.classes = None\n",
    "    self.labels_IO = None\n",
    "    self.labels_attribute = None\n",
    "    self.W_attribute = None\n",
    "\n",
    "    self.features_blobs = []\n",
    "    self.load_labels()\n",
    "    self.model = self.load_model()\n",
    "    self.tf = self.returnTF()\n",
    "\n",
    "    self.params = list(self.model.parameters())\n",
    "    self.weight_softmax = self.params[-2].data.numpy()\n",
    "    self.weight_softmax[self.weight_softmax<0] = 0\n",
    "\n",
    "    self.frame_names = []\n",
    "    self.io_score = []\n",
    "    self.io = []\n",
    "    self.prob_cat_1 = []\n",
    "    self.cat_1 = []\n",
    "    self.prob_cat_2 = []\n",
    "    self.cat_2 = []\n",
    "    self.prob_cat_3 = []\n",
    "    self.cat_3 = []\n",
    "    self.prob_cat_4 = []\n",
    "    self.cat_4 = []\n",
    "    self.prob_cat_5 = []\n",
    "    self.cat_5 = []\n",
    "\n",
    "  def load_labels(self):\n",
    "    c = list()\n",
    "    with open(self.CATEGORIES_PLACES) as class_file:\n",
    "      for line in class_file:\n",
    "        c.append(line.strip().split(' ')[0][3:])\n",
    "    self.classes = tuple(c)\n",
    "\n",
    "    with open(self.IO_PLACES) as f:\n",
    "      lines = f.readlines()\n",
    "      labels_IO = []\n",
    "      for line in lines:\n",
    "        items = line.rstrip().split()\n",
    "        labels_IO.append(int(items[-1]) -1) # 0 is indoor, 1 is outdoor\n",
    "    self.labels_IO = np.array(labels_IO)\n",
    "\n",
    "    with open(self.LABELS_SUNATTRIBUTE) as f:\n",
    "      lines = f.readlines()\n",
    "      self.labels_attribute = [item.rstrip() for item in lines]\n",
    "\n",
    "    self.W_attribute = np.load(self.SCENE_ATTRIBUTE_NPY)\n",
    "\n",
    "  def load_model(self):#TODO\n",
    "    # this model has a last conv feature map as 14x14\n",
    "    import models.wideresnet as wideresnet\n",
    "    model = wideresnet.resnet18(num_classes=365)\n",
    "    checkpoint = torch.load(\"./models/wideresnet18_places365.pth.tar\", map_location=lambda storage, loc: storage)\n",
    "    state_dict = {str.replace(k,'module.',''): v for k,v in checkpoint['state_dict'].items()}\n",
    "    model.load_state_dict(state_dict)\n",
    "    \n",
    "    # hacky way to deal with the upgraded batchnorm2D and avgpool layers...\n",
    "    for i, (name, module) in enumerate(model._modules.items()):\n",
    "        module = self.recursion_change_bn(model)\n",
    "    model.avgpool = torch.nn.AvgPool2d(kernel_size=14, stride=1, padding=0)\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    # the following is deprecated, everything is migrated to python36\n",
    "\n",
    "    ## if you encounter the UnicodeDecodeError when use python3 to load the model, add the following line will fix it. Thanks to @soravux\n",
    "    #from functools import partial\n",
    "    #import pickle\n",
    "    #pickle.load = partial(pickle.load, encoding=\"latin1\")\n",
    "    #pickle.Unpickler = partial(pickle.Unpickler, encoding=\"latin1\")\n",
    "    #model = torch.load(model_file, map_location=lambda storage, loc: storage, pickle_module=pickle)\n",
    "\n",
    "    model.eval()\n",
    "    # hook the feature extractor\n",
    "    features_names = ['layer4','avgpool'] # this is the last conv layer of the resnet\n",
    "    for name in features_names:\n",
    "        model._modules.get(name).register_forward_hook(self.hook_feature)\n",
    "    return model\n",
    "\n",
    "  # hacky way to deal with the Pytorch 1.0 update\n",
    "  def recursion_change_bn(self, module):\n",
    "    if isinstance(module, torch.nn.BatchNorm2d):\n",
    "      module.track_running_stats = 1\n",
    "    else:\n",
    "      for i, (name, module1) in enumerate(module._modules.items()):\n",
    "        module1 = self.recursion_change_bn(module1)\n",
    "    return module\n",
    "\n",
    "  def hook_feature(self, module, input, output):\n",
    "    self.features_blobs.append(np.squeeze(output.data.cpu().numpy()))\n",
    "\n",
    "  def returnCAM(self, feature_conv, weight_softmax, class_idx):\n",
    "    # generate the class activation maps upsample to 256x256\n",
    "    size_upsample = (256, 256)\n",
    "    nc, h, w = feature_conv.shape\n",
    "    output_cam = []\n",
    "    for idx in class_idx:\n",
    "        cam = weight_softmax[class_idx].dot(feature_conv.reshape((nc, h*w)))\n",
    "        cam = cam.reshape(h, w)\n",
    "        cam = cam - np.min(cam)\n",
    "        cam_img = cam / np.max(cam)\n",
    "        cam_img = np.uint8(255 * cam_img)\n",
    "        output_cam.append(cv2.resize(cam_img, size_upsample))\n",
    "    return output_cam\n",
    "\n",
    "  def returnTF(self):\n",
    "  # load the image transformer\n",
    "    tf = trn.Compose([\n",
    "      trn.Resize((224,224)),\n",
    "      trn.ToTensor(),\n",
    "      trn.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    return tf\n",
    "\n",
    "  def handleNextImage(self, frameName, frameData):\n",
    "    self.frame_names.append(frameName)\n",
    "    imageCopy = frameData.copy()\n",
    "    image = Image.fromarray(cv2.cvtColor(frameData, cv2.COLOR_BGR2RGB))\n",
    "    \n",
    "    input_img = V(self.tf(image).unsqueeze(0))   \n",
    "    logit = self.model.forward(input_img)\n",
    "    h_x = F.softmax(logit, 1).data.squeeze()\n",
    "    probs, idx = h_x.sort(0, True)\n",
    "    probs = probs.numpy()\n",
    "    idx = idx.numpy() \n",
    "\n",
    "    io_image = np.mean(self.labels_IO[idx[:10]])\n",
    "    self.io_score.append(io_image)\n",
    "    if io_image < 0.5:\n",
    "      self.io.append(\"indoor\")\n",
    "    else:\n",
    "      self.io.append(\"outdoor\")\n",
    "    \n",
    "    self.prob_cat_1.append(probs[0])\n",
    "    self.cat_1.append(self.classes[idx[0]])\n",
    "    self.prob_cat_2.append(probs[1])\n",
    "    self.cat_2.append(self.classes[idx[1]])\n",
    "    self.prob_cat_3.append(probs[2])\n",
    "    self.cat_3.append(self.classes[idx[2]])\n",
    "    self.prob_cat_4.append(probs[3])\n",
    "    self.cat_4.append(self.classes[idx[3]])\n",
    "    self.prob_cat_5.append(probs[4])\n",
    "    self.cat_5.append(self.classes[idx[4]])\n",
    "\n",
    "    loc1 = \"Location 1: {} ({})\". format(self.cat_1[len(self.cat_1)-1], \"{:.2f}\".format(self.prob_cat_1[len(self.prob_cat_1)-1]))\n",
    "    loc2 = \"Location 2: {} ({})\". format(self.cat_2[len(self.cat_2)-1], \"{:.2f}\".format(self.prob_cat_2[len(self.prob_cat_2)-1]))\n",
    "    loc3 = \"Location 3: {} ({})\". format(self.cat_3[len(self.cat_3)-1], \"{:.2f}\".format(self.prob_cat_3[len(self.prob_cat_3)-1]))\n",
    "    cat = \"Category: {}\".format(self.io[len(self.io)-1])\n",
    "\n",
    "    color = (255,255,255)\n",
    "    cv2.putText(imageCopy, loc1, (20, 40), cv2.FONT_HERSHEY_SIMPLEX,0.5,color,1,cv2.LINE_AA, )\n",
    "    cv2.putText(imageCopy, loc2, (20, 60), cv2.FONT_HERSHEY_SIMPLEX,0.5,color,1,cv2.LINE_AA, )\n",
    "    cv2.putText(imageCopy, loc3, (20, 80), cv2.FONT_HERSHEY_SIMPLEX,0.5,color,1,cv2.LINE_AA, )\n",
    "    cv2.putText(imageCopy, cat, (20, 110), cv2.FONT_HERSHEY_SIMPLEX,0.5,color,1,cv2.LINE_AA, )\n",
    "    \n",
    "    cv2.imwrite(f\"{self.outputPathImages}{frameName}\", imageCopy)\n",
    "\n",
    "  def dataToCSV(self):\n",
    "    df = pd.DataFrame(list(zip(self.frame_names, self.io_score, self.io, self.prob_cat_1, self.cat_1, self.prob_cat_2, self.cat_2, self.prob_cat_3, self.cat_3, self.prob_cat_4, self.cat_4, self.prob_cat_5, self.cat_5)))\n",
    "    df.columns = ['Name', 'io_score', 'io', 'prob_cat_1', 'cat_1', 'prob_cat_2', 'cat_2', 'prob_cat_3', 'cat_3', 'prob_cat_4', 'cat_4', 'prob_cat_5', 'cat_5']\n",
    "    df.to_csv(f\"{self.outputPathCSV}{self.gameName}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectDetection:\n",
    "\n",
    "  def __init__(self, gameName, outputPath):\n",
    "    self.method = \"ObjectDetection\"\n",
    "    self.gameName = gameName\n",
    "    self.outputPathCSV = f\"{outputPath}{self.method}\\\\\"\n",
    "    self.outputPathImages = f\"{self.outputPathCSV}\\\\{self.gameName}\\\\\"\n",
    "\n",
    "    self.threshold = 0.5\n",
    "    self.createImages = True\n",
    "    self.predictor = None\n",
    "    self.cfg = None\n",
    "    self.classDict = None\n",
    "    self.allDic = []\n",
    "    \n",
    "    self.checkGPU()\n",
    "    self.loadClasses()\n",
    "   \n",
    "  def checkGPU():\n",
    "    # If there's a GPU available...\n",
    "    if torch.cuda.is_available():    \n",
    "        # Tell PyTorch to use the GPU.    \n",
    "        device = torch.device(\"cuda\")\n",
    "\n",
    "        print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "        print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "    # If not...\n",
    "    else:\n",
    "        #device = torch.device(\"cpu\")\n",
    "        raise Exception(\"No GPU available, using the CPU instead.\")\n",
    "\n",
    "  #all underlined methods and classes are imports from detectron\n",
    "  def configDetectron(self):\n",
    "    self.cfg = get_cfg()\n",
    "\n",
    "    # add project-specific config (e.g., TensorMask) here if you're not running a model in detectron2's core library\n",
    "    self.cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
    "    self.cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = self.threshold  # set threshold for this model\n",
    "\n",
    "    # Find a model from detectron2's model zoo. You can use the https://dl.fbaipublicfiles... url as well\n",
    "    self.cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
    "\n",
    "    self.predictor = DefaultPredictor(cfg)\n",
    "\n",
    "  def loadClasses(self):\n",
    "    self.classDict = {0: u'__background__',\n",
    "      1: u'person',\n",
    "      2: u'bicycle',\n",
    "      3: u'car',\n",
    "      4: u'motorcycle',\n",
    "      5: u'airplane',\n",
    "      6: u'bus',\n",
    "      7: u'train',\n",
    "      8: u'truck',\n",
    "      9: u'boat',\n",
    "      10: u'traffic light',\n",
    "      11: u'fire hydrant',\n",
    "      12: u'stop sign',\n",
    "      13: u'parking meter',\n",
    "      14: u'bench',\n",
    "      15: u'bird',\n",
    "      16: u'cat',\n",
    "      17: u'dog',\n",
    "      18: u'horse',\n",
    "      19: u'sheep',\n",
    "      20: u'cow',\n",
    "      21: u'elephant',\n",
    "      22: u'bear',\n",
    "      23: u'zebra',\n",
    "      24: u'giraffe',\n",
    "      25: u'backpack',\n",
    "      26: u'umbrella',\n",
    "      27: u'handbag',\n",
    "      28: u'tie',\n",
    "      29: u'suitcase',\n",
    "      30: u'frisbee',\n",
    "      31: u'skis',\n",
    "      32: u'snowboard',\n",
    "      33: u'sports ball',\n",
    "      34: u'kite',\n",
    "      35: u'baseball bat',\n",
    "      36: u'baseball glove',\n",
    "      37: u'skateboard',\n",
    "      38: u'surfboard',\n",
    "      39: u'tennis racket',\n",
    "      40: u'bottle',\n",
    "      41: u'wine glass',\n",
    "      42: u'cup',\n",
    "      43: u'fork',\n",
    "      44: u'knife',\n",
    "      45: u'spoon',\n",
    "      46: u'bowl',\n",
    "      47: u'banana',\n",
    "      48: u'apple',\n",
    "      49: u'sandwich',\n",
    "      50: u'orange',\n",
    "      51: u'broccoli',\n",
    "      52: u'carrot',\n",
    "      53: u'hot dog',\n",
    "      54: u'pizza',\n",
    "      55: u'donut',\n",
    "      56: u'cake',\n",
    "      57: u'chair',\n",
    "      58: u'couch',\n",
    "      59: u'potted plant',\n",
    "      60: u'bed',\n",
    "      61: u'dining table',\n",
    "      62: u'toilet',\n",
    "      63: u'tv',\n",
    "      64: u'laptop',\n",
    "      65: u'mouse',\n",
    "      66: u'remote',\n",
    "      67: u'keyboard',\n",
    "      68: u'cell phone',\n",
    "      69: u'microwave',\n",
    "      70: u'oven',\n",
    "      71: u'toaster',\n",
    "      72: u'sink',\n",
    "      73: u'refrigerator',\n",
    "      74: u'book',\n",
    "      75: u'clock',\n",
    "      76: u'vase',\n",
    "      77: u'scissors',\n",
    "      78: u'teddy bear',\n",
    "      79: u'hair drier',\n",
    "      80: u'toothbrush'}\n",
    "\n",
    "  def createBaseDict(self):\n",
    "    baseDict = {v: k for k,v in self.classDict.items()}\n",
    "    del baseDict[\"__background__\"]\n",
    "    for item in baseDict:\n",
    "      baseDict[item] = 0\n",
    "    return baseDict\n",
    "\n",
    "  def getObjectsPerFrame(self, classes):\n",
    "    frameDict = self.createBaseDict()\n",
    "    for item in classes:\n",
    "      classNumber = item.item() + 1\n",
    "      className = self.classDict[classNumber]\n",
    "\n",
    "      frameDict[className] = frameDict[className] + 1\n",
    "\n",
    "    return frameDict\n",
    "\n",
    "  def handleNextImage(self, frameName, frameData):\n",
    "    outputs = self.predictor(frameData)\n",
    "    instances = outputs[\"instances\"].pred_classes\n",
    "    frame = self.getObjectsPerFrame(instances)\n",
    "    frame[\"_id\"] = frameName.split(\".\")[0]\n",
    "\n",
    "    self.allDic.append(frameName)\n",
    "\n",
    "    if self.createImages:\n",
    "      v = Visualizer(frameData[:, :, ::-1], MetadataCatalog.get(self.cfg.DATASETS.TRAIN[0]), scale=1.0)\n",
    "      out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "\n",
    "      cv2.imwrite(f\"{self.outputPathImages}{frameName}\", out.get_image()[:, :, ::-1])\n",
    "   \n",
    "\n",
    "  def dataToCSV(self):\n",
    "    data = pd.DataFrame.from_dict(self.allDic)\n",
    "    data = data.reindex(sorted(data.columns), axis=1)\n",
    "\n",
    "    #data = data.sort_values(\"_id\")\n",
    "    data[\"sum\"] = data.loc[:, \"airplane\":\"zebra\"].sum(1)\n",
    "    data.to_csv(f\"{self.outputPathCSV}{self.gameName}.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reportProgress(count, end, videoName, startTime):\n",
    "  if count == 0:\n",
    "    print(f'\\nStarting to process {videoName}...')\n",
    "  else:\n",
    "    passedTime = time.time() - startTime\n",
    "    progress = count / end\n",
    "    totalTime = passedTime / progress\n",
    "    remainingTime = totalTime - passedTime\n",
    "\n",
    "    hours = math.floor(remainingTime / 3600)\n",
    "    minutes = math.floor((remainingTime - hours * 3600) / 60)\n",
    "    \n",
    "    print(f'\\rProcessing {videoName}: {round(progress * 100, 2)}% -- {hours}hrs {minutes}min remaining.', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input folder is ok.\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\AgeGender\\Diablo2 contains files\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\AgeGender\\Diablo2.csv not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\AgeGender\\Die_Siedler3 contains files\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\AgeGender\\Die_Siedler3.csv not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\AgeGender\\Elden_Ring contains files\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\AgeGender\\Elden_Ring.csv not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\AgeGender\\Half_Life2 contains files\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\AgeGender\\Half_Life2.csv not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\AgeGender\\Little_Nightmares contains files\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\AgeGender\\Little_Nightmares.csv not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\AgeGender\\NFSMW contains files\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\AgeGender\\NFSMW.csv not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\AgeGender\\Starcraft2 contains files\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\AgeGender\\Starcraft2.csv not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\AgeGender\\Super_Mario_World contains files\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\AgeGender\\Super_Mario_World.csv not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\AverageColor\\Diablo2_average_color_for_each_frame.svg not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\AverageColor\\Diablo2_average_color_row_data.csv not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\AverageColor\\Die_Siedler3_average_color_for_each_frame.svg not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\AverageColor\\Die_Siedler3_average_color_row_data.csv not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\AverageColor\\Elden_Ring_average_color_for_each_frame.svg not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\AverageColor\\Elden_Ring_average_color_row_data.csv not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\AverageColor\\Half_Life2_average_color_for_each_frame.svg not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\AverageColor\\Half_Life2_average_color_row_data.csv not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\AverageColor\\Little_Nightmares_average_color_for_each_frame.svg not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\AverageColor\\Little_Nightmares_average_color_row_data.csv not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\AverageColor\\NFSMW_average_color_for_each_frame.svg not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\AverageColor\\NFSMW_average_color_row_data.csv not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\AverageColor\\Starcraft2_average_color_for_each_frame.svg not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\AverageColor\\Starcraft2_average_color_row_data.csv not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\AverageColor\\Super_Mario_World_average_color_for_each_frame.svg not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\AverageColor\\Super_Mario_World_average_color_row_data.csv not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\Brightness\\Diablo2_Brightness.csv not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\Brightness\\Diablo2_Graph.svg not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\Brightness\\Diablo2_Statistics.csv not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\Brightness\\Die_Siedler3_Brightness.csv not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\Brightness\\Die_Siedler3_Graph.svg not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\Brightness\\Die_Siedler3_Statistics.csv not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\Brightness\\Elden_Ring_Brightness.csv not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\Brightness\\Elden_Ring_Graph.svg not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\Brightness\\Elden_Ring_Statistics.csv not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\Brightness\\Half_Life2_Brightness.csv not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\Brightness\\Half_Life2_Graph.svg not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\Brightness\\Half_Life2_Statistics.csv not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\Brightness\\Little_Nightmares_Brightness.csv not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\Brightness\\Little_Nightmares_Graph.svg not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\Brightness\\Little_Nightmares_Statistics.csv not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\Brightness\\NFSMW_Brightness.csv not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\Brightness\\NFSMW_Graph.svg not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\Brightness\\NFSMW_Statistics.csv not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\Brightness\\Starcraft2_Brightness.csv not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\Brightness\\Starcraft2_Graph.svg not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\Brightness\\Starcraft2_Statistics.csv not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\Brightness\\Super_Mario_World_Brightness.csv not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\Brightness\\Super_Mario_World_Graph.svg not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\Brightness\\Super_Mario_World_Statistics.csv not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\DominantColor\\Diablo2_dominant_color_for_each_frame.svg not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\DominantColor\\Diablo2_dominant_color_row_data.csv not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\DominantColor\\Die_Siedler3_dominant_color_for_each_frame.svg not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\DominantColor\\Die_Siedler3_dominant_color_row_data.csv not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\DominantColor\\Elden_Ring_dominant_color_for_each_frame.svg not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\DominantColor\\Elden_Ring_dominant_color_row_data.csv not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\DominantColor\\Half_Life2_dominant_color_for_each_frame.svg not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\DominantColor\\Half_Life2_dominant_color_row_data.csv not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\DominantColor\\Little_Nightmares_dominant_color_for_each_frame.svg not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\DominantColor\\Little_Nightmares_dominant_color_row_data.csv not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\DominantColor\\NFSMW_dominant_color_for_each_frame.svg not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\DominantColor\\NFSMW_dominant_color_row_data.csv not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\DominantColor\\Starcraft2_dominant_color_for_each_frame.svg not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\DominantColor\\Starcraft2_dominant_color_row_data.csv not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\DominantColor\\Super_Mario_World_dominant_color_for_each_frame.svg not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\DominantColor\\Super_Mario_World_dominant_color_row_data.csv not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\Emotion\\Diablo2 contains files\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\Emotion\\Diablo2.csv not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\Emotion\\Die_Siedler3 contains files\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\Emotion\\Die_Siedler3.csv not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\Emotion\\Elden_Ring contains files\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\Emotion\\Elden_Ring.csv not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\Emotion\\Half_Life2 contains files\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\Emotion\\Half_Life2.csv not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\Emotion\\Little_Nightmares contains files\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\Emotion\\Little_Nightmares.csv not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\Emotion\\NFSMW contains files\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\Emotion\\NFSMW.csv not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\Emotion\\Starcraft2 contains files\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\Emotion\\Starcraft2.csv not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\Emotion\\Super_Mario_World contains files\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\Emotion\\Super_Mario_World.csv not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\Location\\Diablo2 contains files\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\Location\\Diablo2.csv not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\Location\\Die_Siedler3 contains files\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\Location\\Die_Siedler3.csv not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\Location\\Elden_Ring contains files\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\Location\\Elden_Ring.csv not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\Location\\Half_Life2 contains files\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\Location\\Half_Life2.csv not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\Location\\Little_Nightmares contains files\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\Location\\Little_Nightmares.csv not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\Location\\NFSMW contains files\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\Location\\NFSMW.csv not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\Location\\Starcraft2 contains files\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\Location\\Starcraft2.csv not in games or not a directory\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\Location\\Super_Mario_World contains files\n",
      "WARNING: C:\\Users\\User\\Documents\\GitHub\\dhcv\\output\\Location\\Super_Mario_World.csv not in games or not a directory\n",
      "Output folder is ok.\n",
      "Processing Die_Siedler3: 19.35% -- 0hrs 0min remaining."
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a real number, not 'tuple'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 43\u001b[0m\n\u001b[0;32m     41\u001b[0m brightnessAnalyzer\u001b[39m.\u001b[39mdataToCSV()\n\u001b[0;32m     42\u001b[0m dominantColor\u001b[39m.\u001b[39mdataToCSV()\n\u001b[1;32m---> 43\u001b[0m averageColor\u001b[39m.\u001b[39;49mdataToCSV()\n\u001b[0;32m     44\u001b[0m emotionDetection\u001b[39m.\u001b[39mdataToCSV()\n\u001b[0;32m     45\u001b[0m locationDetection\u001b[39m.\u001b[39mdataToCSV()\n",
      "Cell \u001b[1;32mIn[26], line 47\u001b[0m, in \u001b[0;36mAverageColor.dataToCSV\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[39mfor\u001b[39;00m idx, frame_name \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maverageColorArr):\n\u001b[0;32m     46\u001b[0m   average_color_dict[frame_name] \u001b[39m=\u001b[39m []\n\u001b[1;32m---> 47\u001b[0m   average_color_dict[frame_name]\u001b[39m.\u001b[39mappend(\u001b[39mint\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maverageColorArr[idx]))\n\u001b[0;32m     48\u001b[0m   average_color_dict[frame_name]\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframeNames[idx])\n\u001b[0;32m     50\u001b[0m average_color_row_data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame\u001b[39m.\u001b[39mfrom_dict(average_color_dict, orient\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m'\u001b[39m, columns\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mRGB\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mframe\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "\u001b[1;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a real number, not 'tuple'"
     ]
    }
   ],
   "source": [
    "config = Configuration()\n",
    "config.checkInputFolder()\n",
    "config.checkOutputFolder()\n",
    "\n",
    "reportCount = 2 #TODO change to 50\n",
    "count = 0\n",
    "end = config.getFileLength()\n",
    "startTime = time.time()\n",
    "\n",
    "for gameName in config.getGameNames():\n",
    "  ageGender = AgeGender(gameName, config.getOutputRoot())\n",
    "  brightnessAnalyzer = BrightnessAnalyzer(gameName, config.getOutputRoot())\n",
    "  dominantColor = DominantColor( gameName, config.getOutputRoot())\n",
    "  averageColor = AverageColor(gameName, config.getOutputRoot())\n",
    "  emotionDetection = EmotionDetection(gameName, config.getOutputRoot())\n",
    "  locationDetection = LocationDetection(gameName, config.getOutputRoot())\n",
    "  #objectDetection = ObjectDetection(gameName, config.getOutputRoot())\n",
    "\n",
    "  framesList = os.listdir(f\"{config.getInputRoot()}{gameName}\\\\\")\n",
    "  framesList = natsorted(framesList, alg=ns.PATH | ns.IGNORECASE)\n",
    "\n",
    "  for frameName in framesList:\n",
    "    count += 1\n",
    "\n",
    "    if not frameName.split(\".\")[-1].lower() in {\"jpeg\", \"jpg\", \"png\"}:\n",
    "      continue\n",
    "    \n",
    "    frameData = cv2.imread(f\"{config.getInputRoot()}{gameName}\\\\{frameName}\")\n",
    "    \n",
    "    ageGender.handleNextImage(frameName, frameData)\n",
    "    brightnessAnalyzer.handleNextImage(frameName, frameData)\n",
    "    dominantColor.handleNextImage(frameName, frameData)\n",
    "    averageColor.handleNextImage(frameName, frameData)\n",
    "    emotionDetection.handleNextImage(frameName, frameData)\n",
    "    locationDetection.handleNextImage(frameName, frameData)\n",
    "    #objectDetection.handleNextImage(frameName, frameData)\n",
    "  \n",
    "    if count % reportCount == 0:\n",
    "      reportProgress(count, end, gameName, startTime)\n",
    "    \n",
    "  #save data as csv\n",
    "  ageGender.dataToCSV()\n",
    "  brightnessAnalyzer.dataToCSV()\n",
    "  dominantColor.dataToCSV()\n",
    "  averageColor.dataToCSV()\n",
    "  emotionDetection.dataToCSV()\n",
    "  locationDetection.dataToCSV()\n",
    "  #objectDetection.dataToCSV()\n",
    "\n",
    "  #generate color bars\n",
    "  dominantColor.generateColorBars()\n",
    "  averageColor.generateColorBars()\n",
    "\n",
    "  #generate greyscale graph\n",
    "  brightnessAnalyzer.generateGreyScaleGraph()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d6540d84677c43a09ba77dca21a04ef847cd39b5846de4b170515afa4945c7b0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
